{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Pytorch\n",
    "\n",
    "In machine learning and deep learning, tensors are multi-dimensional arrays with a uniform type (called a dtype). They are similar to NumPy arrays. All tensors are immutable like Python numbers and strings: you can never update the contents of a tensor, only create a new one.\n",
    "\n",
    "Tensors are multidimensional arrays that can be used on a GPU to accelerate numerical operations and support automatic differentiation. Numpy arrays are ordered lists of numbers that can only be used on a CPU and do not support automatic differentiation.\n",
    "\n",
    "Automatic differentiation is a technique used to compute the derivatives of a function with respect to its input variables. It is used in machine learning and deep learning to compute gradients for optimization algorithms such as stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "z = torch.zeros(5, 3)\n",
    "print(z)\n",
    "print(z.dtype) # default data type is float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]], dtype=torch.int16)\n",
      "torch.int16\n"
     ]
    }
   ],
   "source": [
    "z = torch.zeros((5,3), dtype=torch.int16)\n",
    "print(z)\n",
    "print(z.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5189, 0.4580],\n",
      "        [0.2942, 0.2120],\n",
      "        [0.1669, 0.5207],\n",
      "        [0.1118, 0.5372],\n",
      "        [0.5938, 0.2800]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(17297)\n",
    "z = torch.rand(5,2)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4., 4.],\n",
      "        [4., 4.],\n",
      "        [4., 4.],\n",
      "        [4., 4.],\n",
      "        [4., 4.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5,2)\n",
    "b = torch.ones(5,2) * 3\n",
    "print(a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1v\n"
     ]
    }
   ],
   "source": [
    "c = torch.ones(5,3)\n",
    "try: \n",
    "    print(a+c) # shapes are different therefore error\n",
    "except:\n",
    "    print('The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1v')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5938)\n"
     ]
    }
   ],
   "source": [
    "print(torch.max(z))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd\n",
    "\n",
    "#### Loss Function\n",
    "\n",
    "you can intuitively calculate the loss function because the gradients are calculated while you multiply your weights with the inputs and apply a tanh function to find the output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.2270)\n"
     ]
    }
   ],
   "source": [
    "x       = torch.randn(1,10)\n",
    "prev_h  = torch.randn(1,20)\n",
    "W_h     = torch.randn(20,20)\n",
    "W_x     = torch.randn(20,10)\n",
    "\n",
    "i2h = torch.mm(W_x, x.t())\n",
    "h2h = torch.mm(W_h, prev_h.t())\n",
    "\n",
    "next_h = i2h + h2h\n",
    "next_h = next_h.tanh()\n",
    "\n",
    "loss = next_h.sum()\n",
    "# loss.backward()\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple pytorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F # for activation function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LeNet-5 model\n",
    "\n",
    "One of the earliest convolutional NNs. Scans the input and analyzes the hand written images.\n",
    "\n",
    "- input image is 32x32\n",
    "\n",
    "- C1: a convolutional layer to extract features from the image and generates a map of where it saw each of its learned features. 6@28x28\n",
    "\n",
    "- s2: The output of C1 is then downsized. 6@14x14\n",
    "\n",
    "- C3: is another convolutional layer, looking for combination of features. 16@10x10\n",
    "\n",
    "- f5: layer of 120n\n",
    "\n",
    "- f6: layer of 84n\n",
    "\n",
    "- output: 10n for the 10 digits\n",
    "\n",
    "\n",
    "here is how to express this nn in pytorch:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lenet5.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        # 1 input image channel (black & white), 6 output channels, 3x3 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This demonstrates the structure of a typical PyTorch model:\n",
    "* It inherits from `torch.nn.Module` - modules may be nested - in fact, even the `Conv2d` and `Linear` layer classes inherit from `torch.nn.Module`.\n",
    "* A model will have an `__init__()` function, where it instantiates its layers, and loads any data artifacts it might need (e.g., an NLP model might load a vocabulary).\n",
    "* A model will have a `forward()` function. This is where the actual computation happens: An input is passed through the network layers and various functions to generate an output.\n",
    "* Other than that, you can build out your model class like any other Python class, adding whatever properties and methods you need to support your model's computation.\n",
    "\n",
    "Let's instantiate this object and run a sample input through it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=576, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n",
      "\n",
      "Image batch shape:\n",
      "torch.Size([1, 1, 32, 32])\n",
      "\n",
      "Raw output:\n",
      "tensor([[-0.0456,  0.0107,  0.0164, -0.0700,  0.1095, -0.0199,  0.0120,  0.0640,\n",
      "         -0.0942,  0.0922]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "net = LeNet()\n",
    "print(net)                         # what does the object tell us about itself?\n",
    "\n",
    "input = torch.rand(1, 1, 32, 32)   # stand-in for a 32x32 black & white image\n",
    "print('\\nImage batch shape:')\n",
    "print(input.shape)\n",
    "\n",
    "output = net(input)                # we don't call forward() directly\n",
    "print('\\nRaw output:')\n",
    "print(output)\n",
    "print(output.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's train the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the input datasets into tensors and normalize them\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.2%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.7%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
     ]
    }
   ],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zade/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/zade/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ship  bird   dog plane\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and calculate loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zade/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/zade/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 2.232\n",
      "[1,  4000] loss: 1.845\n",
      "[1,  6000] loss: 1.698\n",
      "[1,  8000] loss: 1.589\n",
      "[1, 10000] loss: 1.509\n",
      "[1, 12000] loss: 1.467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zade/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/zade/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2,  2000] loss: 1.395\n",
      "[2,  4000] loss: 1.379\n",
      "[2,  6000] loss: 1.354\n",
      "[2,  8000] loss: 1.330\n",
      "[2, 10000] loss: 1.321\n",
      "[2, 12000] loss: 1.285\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimizea\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zade/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/zade/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 55 %\n"
     ]
    }
   ],
   "source": [
    "# test the model on the data it hasn't been trained on\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment\n",
    "\n",
    "- use torchscript to build functions, classes and data structure with python.\n",
    "- pytorch JIT: torchscript is going to be used by JIT (just in time compiler)\n",
    "\n",
    "you save and then load the model for production use. this can be used in C++ too.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Tensors\n",
    "\n",
    "- math and logic\n",
    "- copying tensors\n",
    "- moving to gpu\n",
    "- manipulating tensor shapes\n",
    "- pytorch-numpy bridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3])\n",
      "tensor([[[0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(2,2,3)\n",
    "print(x.shape)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3])\n",
      "tensor([[[0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "# create a new empty tensor with the same size as x\n",
    "empty_like_x = torch.empty_like(x)\n",
    "print(empty_like_x.shape)\n",
    "print(empty_like_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3])\n",
      "tensor([[[0.1865, 0.9334, 0.0312],\n",
      "         [0.2435, 0.9302, 0.3645]],\n",
      "\n",
      "        [[0.5234, 0.1820, 0.2429],\n",
      "         [0.8434, 0.8645, 0.7145]]])\n"
     ]
    }
   ],
   "source": [
    "empty_rand_x = torch.rand_like(x)\n",
    "print(empty_rand_x.shape)\n",
    "print(empty_rand_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 12,  13],\n",
       "        [100, 200]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.tensor creates a copy of data\n",
    "some_constant = torch.tensor([\n",
    "    [12,13],\n",
    "    [100, 200]\n",
    "])\n",
    "some_constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.int32\n"
     ]
    }
   ],
   "source": [
    "# convert data type\n",
    "a = torch.ones(2,3)\n",
    "print(a.dtype)\n",
    "c = a.to(torch.int32)\n",
    "print(c.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 1.],\n",
      "         [1., 1.],\n",
      "         [1., 1.]],\n",
      "\n",
      "        [[1., 1.],\n",
      "         [1., 1.],\n",
      "         [1., 1.]]])\n",
      "tensor([[[2., 2.],\n",
      "         [2., 2.],\n",
      "         [2., 2.]],\n",
      "\n",
      "        [[2., 2.],\n",
      "         [2., 2.],\n",
      "         [2., 2.]]])\n",
      "tensor([[[2., 2.],\n",
      "         [2., 2.],\n",
      "         [2., 2.]],\n",
      "\n",
      "        [[2., 2.],\n",
      "         [2., 2.],\n",
      "         [2., 2.]]])\n",
      "tensor([[[2., 2.],\n",
      "         [2., 2.],\n",
      "         [2., 2.]],\n",
      "\n",
      "        [[2., 2.],\n",
      "         [2., 2.],\n",
      "         [2., 2.]]])\n"
     ]
    }
   ],
   "source": [
    "a =     torch.ones(2, 3, 2)\n",
    "print(a)\n",
    "\n",
    "b = a * torch.ones(   3, 2) * 2 # 3rd & 2nd dims identical to a, dim 1 absent\n",
    "print(b)\n",
    "\n",
    "c = a * torch.ones(   3, 1) * 2 # 3rd dim = 1, 2nd dim identical to a\n",
    "print(c)\n",
    "\n",
    "d = a * torch.ones(   1, 2) * 2 # 3rd dim identical to a, 2nd dim = 1\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common functions:\n",
      "tensor([[0.3714, 0.2766, 0.8346, 0.2991],\n",
      "        [0.1154, 0.4826, 0.8868, 0.0146]])\n",
      "tensor([[-0., -0., 1., 1.],\n",
      "        [-0., -0., -0., -0.]])\n",
      "tensor([[-1., -1.,  0.,  0.],\n",
      "        [-1., -1., -1., -1.]])\n",
      "tensor([[-0.3714, -0.2766,  0.5000,  0.2991],\n",
      "        [-0.1154, -0.4826, -0.5000, -0.0146]])\n"
     ]
    }
   ],
   "source": [
    "# common functions\n",
    "a = torch.rand(2, 4) * 2 - 1\n",
    "print('Common functions:')\n",
    "print(torch.abs(a))\n",
    "print(torch.ceil(a))\n",
    "print(torch.floor(a))\n",
    "print(torch.clamp(a, -0.5, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sine and arcsine:\n",
      "tensor([0.0000, 0.7854, 1.5708, 2.3562])\n",
      "tensor([0.0000, 0.7071, 1.0000, 0.7071])\n",
      "tensor([0.0000, 0.7854, 1.5708, 0.7854])\n"
     ]
    }
   ],
   "source": [
    "# trigonometric functions and their inverses\n",
    "angles = torch.tensor([0, math.pi / 4, math.pi / 2, 3 * math.pi / 4])\n",
    "sines = torch.sin(angles)\n",
    "inverses = torch.asin(sines)\n",
    "print('\\nSine and arcsine:')\n",
    "print(angles)\n",
    "print(sines)\n",
    "print(inverses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bitwise XOR:\n",
      "tensor([3, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "# bitwise operations\n",
    "print('\\nBitwise XOR:')\n",
    "b = torch.tensor([1, 5, 11])\n",
    "c = torch.tensor([2, 7, 10])\n",
    "print(torch.bitwise_xor(b, c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Broadcasted, element-wise equality comparison:\n",
      "tensor([[ True, False],\n",
      "        [False, False]])\n"
     ]
    }
   ],
   "source": [
    "# comparisons:\n",
    "print('\\nBroadcasted, element-wise equality comparison:')\n",
    "d = torch.tensor([[1., 2.], [3., 4.]])\n",
    "e = torch.ones(1, 2)  # many comparison ops support broadcasting!\n",
    "print(torch.eq(d, e)) # returns a tensor of type bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reduction ops:\n",
      "tensor(4.)\n",
      "4.0\n",
      "tensor(2.5000)\n",
      "tensor(1.2910)\n",
      "tensor(24.)\n",
      "tensor([1, 2])\n"
     ]
    }
   ],
   "source": [
    "# reductions:\n",
    "print('\\nReduction ops:')\n",
    "print(torch.max(d))        # returns a single-element tensor\n",
    "print(torch.max(d).item()) # extracts the value from the returned tensor\n",
    "print(torch.mean(d))       # average\n",
    "print(torch.std(d))        # standard deviation\n",
    "print(torch.prod(d))       # product of all numbers\n",
    "print(torch.unique(torch.tensor([1, 2, 1, 2, 1, 2]))) # filter unique elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vectors & Matrices:\n",
      "tensor([ 0.,  0., -1.])\n",
      "tensor([[0.5862, 0.8831],\n",
      "        [0.0609, 0.7919]])\n",
      "tensor([[1.7586, 2.6493],\n",
      "        [0.1828, 2.3756]])\n",
      "torch.return_types.svd(\n",
      "U=tensor([[ 0.8117,  0.5840],\n",
      "        [ 0.5840, -0.8117]]),\n",
      "S=tensor([3.8563, 0.9578]),\n",
      "V=tensor([[ 0.3979,  0.9174],\n",
      "        [ 0.9174, -0.3979]]))\n"
     ]
    }
   ],
   "source": [
    "# vector and linear algebra operations\n",
    "v1 = torch.tensor([1., 0., 0.])         # x unit vector\n",
    "v2 = torch.tensor([0., 1., 0.])         # y unit vector\n",
    "m1 = torch.rand(2, 2)                   # random matrix\n",
    "m2 = torch.tensor([[3., 0.], [0., 3.]]) # three times identity matrix\n",
    "\n",
    "print('\\nVectors & Matrices:')\n",
    "print(torch.cross(v2, v1)) # negative of z unit vector (v1 x v2 == -v2 x v1)\n",
    "print(m1)\n",
    "m3 = torch.matmul(m1, m2)\n",
    "print(m3)                  # 3 times m1\n",
    "print(torch.svd(m3))       # singular value decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 0.]])\n",
      "tensor([[0.8412, 0.7441],\n",
      "        [0.5659, 0.5166]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2, 2)\n",
    "b = torch.rand(2, 2)\n",
    "c = torch.zeros(2, 2)\n",
    "old_id = id(c)\n",
    "\n",
    "print(c)\n",
    "d = torch.matmul(a, b, out=c)\n",
    "print(c)                # contents of c have changed\n",
    "\n",
    "assert c is d           # test c & d are same object, not just containing equal values\n",
    "assert id(c), old_id    # make sure that our new c is the same object as the old one"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copying Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  1., 561.],\n",
      "        [  1.,   1.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2, 2)\n",
    "b = a\n",
    "\n",
    "a[0][1] = 561  # we change a...\n",
    "print(b)       # ...and b is also altered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[True, True],\n",
      "        [True, True]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2, 2)\n",
    "b = a.clone()\n",
    "\n",
    "assert b is not a      # different objects in memory...\n",
    "print(torch.eq(a, b))  # ...but still with the same contents!\n",
    "\n",
    "a[0][1] = 561          # a changes...\n",
    "print(b)               # ...but b is still all ones"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine you're performing a computation in your model's `forward()` function, where gradients are turned on for everything by default, but you want to pull out some values mid-stream to generate some metrics. In this case, you *don't* want the cloned copy of your source tensor to track gradients - performance is improved with autograd's history tracking turned off. For this, you can use the `.detach()` method on the source tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2851, 0.2596],\n",
      "        [0.9976, 0.9770]], requires_grad=True)\n",
      "tensor([[0.2851, 0.2596],\n",
      "        [0.9976, 0.9770]], grad_fn=<CloneBackward0>)\n",
      "tensor([[0.2851, 0.2596],\n",
      "        [0.9976, 0.9770]])\n",
      "tensor([[0.2851, 0.2596],\n",
      "        [0.9976, 0.9770]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2, 2, requires_grad=True) # turn on autograd\n",
    "print(a)\n",
    "\n",
    "b = a.clone()\n",
    "print(b)\n",
    "\n",
    "c = a.detach().clone()\n",
    "print(c)\n",
    "\n",
    "print(a)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU found; CPU only\n"
     ]
    }
   ],
   "source": [
    "# first check if you have a GPU\n",
    "if torch.cuda.is_available():\n",
    "    print('We have a GPU!')\n",
    "else:\n",
    "    print('No GPU found; CPU only')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorry, CPU only.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    gpu_rand = torch.rand(2, 2, device='cuda')\n",
    "    print(gpu_rand)\n",
    "else:\n",
    "    print('Sorry, CPU only.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "tensor([[0.0690, 0.9839],\n",
      "        [0.7347, 0.3909]])\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    my_device = torch.device('cuda')\n",
    "else:\n",
    "    my_device = torch.device('cpu')\n",
    "print('Device: {}'.format(my_device))\n",
    "\n",
    "x = torch.rand(2, 2, device=my_device)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move a tensor to another device\n",
    "y = torch.rand(2, 2)\n",
    "y = y.to(my_device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulating Tensor Shapes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One case where you might need to change the number of dimensions is passing a single instance of input to your model. PyTorch models generally expect *batches* of input.\n",
    "\n",
    "For example, imagine having a model that works on 3 x 226 x 226 images - a 226-pixel square with 3 color channels. When you load and transform it, you'll get a tensor of shape `(3, 226, 226)`. Your model, though, is expecting input of shape `(N, 3, 226, 226)`, where `N` is the number of images in the batch. So how do you make a batch of one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 226, 226])\n",
      "torch.Size([1, 3, 226, 226])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(3, 226, 226)\n",
    "b = a.unsqueeze(0)\n",
    "\n",
    "print(a.shape)\n",
    "print(b.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `unsqueeze()` method adds a dimension of extent 1. `unsqueeze(0)` adds it as a new zeroth dimension - now you have a batch of one!\n",
    "\n",
    "So if that's *un*squeezing? What do we mean by squeezing? We're taking advantage of the fact that any dimension of extent 1 *does not* change the number of elements in the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20])\n",
      "torch.Size([20])\n",
      "torch.Size([2, 2])\n",
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(1, 20)\n",
    "print(a.shape)\n",
    "# print(a)\n",
    "\n",
    "b = a.squeeze(0)\n",
    "print(b.shape)\n",
    "# print(b)\n",
    "\n",
    "c = torch.rand(2, 2)\n",
    "print(c.shape)\n",
    "\n",
    "d = c.squeeze(0)\n",
    "print(d.shape) # only changes for the dimensions of extent 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1])\n",
      "tensor([[[0.1306, 0.1306],\n",
      "         [0.7423, 0.7423],\n",
      "         [0.8285, 0.8285]],\n",
      "\n",
      "        [[0.1306, 0.1306],\n",
      "         [0.7423, 0.7423],\n",
      "         [0.8285, 0.8285]],\n",
      "\n",
      "        [[0.1306, 0.1306],\n",
      "         [0.7423, 0.7423],\n",
      "         [0.8285, 0.8285]],\n",
      "\n",
      "        [[0.1306, 0.1306],\n",
      "         [0.7423, 0.7423],\n",
      "         [0.8285, 0.8285]]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(4, 3, 2)\n",
    "b = torch.rand(   3)     # trying to multiply a * b will give a runtime error\n",
    "c = b.unsqueeze(1)       # change to a 2-dimensional tensor, adding new dim at the end\n",
    "print(c.shape)\n",
    "print(a * c)             # broadcasting works again!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes you'll want to change the shape of a tensor more radically, while still preserving the number of elements and their contents. One case where this happens is at the interface between a convolutional layer of a model and a linear layer of the model - this is common in image classification models. A convolution kernel will yield an output tensor of shape *features x width x height,* but the following linear layer expects a 1-dimensional input. `reshape()` will do this for you, provided that the dimensions you request yield the same number of elements as the input tensor has:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 20, 20])\n",
      "torch.Size([2400])\n",
      "torch.Size([2400])\n"
     ]
    }
   ],
   "source": [
    "output3d = torch.rand(6, 20, 20)\n",
    "print(output3d.shape)\n",
    "\n",
    "input1d = output3d.reshape(6 * 20 * 20)\n",
    "print(input1d.shape)\n",
    "\n",
    "# can also call it as a method on the torch module:\n",
    "print(torch.reshape(output3d, (6 * 20 * 20,)).shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy bridge\n",
    "\n",
    "If you have existing ML or scientific code with data stored in NumPy ndarrays, you may wish to express that same data as PyTorch tensors, whether to take advantage of PyTorch's GPU acceleration, or its efficient abstractions for building ML models. It's easy to switch between ndarrays and PyTorch tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]]\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "numpy_array = np.ones((2, 3))\n",
    "print(numpy_array)\n",
    "\n",
    "pytorch_tensor = torch.from_numpy(numpy_array)\n",
    "print(pytorch_tensor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conversion can just as easily go the other way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7825, 0.7071, 0.7717],\n",
      "        [0.1704, 0.4550, 0.8205]])\n",
      "[[0.78248334 0.70713633 0.77170706]\n",
      " [0.17042696 0.4550026  0.82054037]]\n"
     ]
    }
   ],
   "source": [
    "pytorch_rand = torch.rand(2, 3)\n",
    "print(pytorch_rand)\n",
    "\n",
    "numpy_rand = pytorch_rand.numpy()\n",
    "print(numpy_rand)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
